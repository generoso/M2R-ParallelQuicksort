# -*- coding: utf-8 -*-
#+STARTUP: overview indent inlineimages
#+TITLE:       Laboratory Notebook for a Multi-Threaded Version of Quicksort
#+AUTHOR:      Generoso Pagano, Arnaud Legrand
#+LANGUAGE:    en
#+TAGS: ARNAUD(a) REPRODUCED(r)

* Intro
This document is based on Arnaud Legrand's journal.org. I stole it and
modified it to conduct a simple performance evaluation of a parallel
implementation of QuickSort, and to play with org mode. In the
[[Experimental%20Reports][Experimental Reports]] section, the tag =ARNAUD= is used to identify
Arnaud's experiments. Among Arnaud's analyses, I tagged with
=REPRODUCED= the analyses I re-executed on my machine.
* Project Overview
This project aims at providing an efficient multi-threaded
implementation of the QuickSort algorithm on multi-core machines. This
document contains some attempts to evaluate the performance of an
implementation of such code.
* General Organization
** src/
This directory comprises the parallel implementation and a standard
Makefile to compile it.
** data/
This is where raw experimental data should go. Each directory entry
comprises a set of experiments and the directory name is based on the
machine name and on the date. For example:
#+begin_src sh :results output :exports both 
echo mkdir data/`hostname`_`date +%F`
#+end_src

#+RESULTS:
: mkdir data/jklaptop_2014-10-24

* Typical usage
** Compilation
A simple makefile with various compilation options is provided in the
src/ directory. Compilation is thus done by running the following command:
#+begin_src sh :results output 
make -C src/
#+end_src

#+RESULTS:
: make: Entering directory '/home/generoso/Dropbox/work/R_course/M2R-ParallelQuicksort/src'
: cc   -g -Wall -Wshadow -Wcast-align -Waggregate-return -Wmissing-prototypes -Wmissing-declarations -Wstrict-prototypes -Wmissing-prototypes -Wmissing-declarations -Wmissing-noreturn -Wpointer-arith -Wwrite-strings -finline-functions -O0 -pthread -lrt -std=c99  -c -o parallelQuicksort.o parallelQuicksort.c
: cc   -g -Wall -Wshadow -Wcast-align -Waggregate-return -Wmissing-prototypes -Wmissing-declarations -Wstrict-prototypes -Wmissing-prototypes -Wmissing-declarations -Wmissing-noreturn -Wpointer-arith -Wwrite-strings -finline-functions -O0 -pthread -lrt -std=c99  parallelQuicksort.o  -o parallelQuicksort 
: make: Leaving directory '/home/generoso/Dropbox/work/R_course/M2R-ParallelQuicksort/src'

Of course, you can clean up everything with:
#+begin_src sh :results output 
make clean -C src/
#+end_src

#+RESULTS:
: make: Entering directory '/home/generoso/Dropbox/work/R_course/M2R-ParallelQuicksort/src'
: rm -f parallelQuicksort *.o *~
: make: Leaving directory '/home/generoso/Dropbox/work/R_course/M2R-ParallelQuicksort/src'

** Running the code
The code is quite simple at the moment and can be run in the following way:
#+begin_src
./src/parallelQuicksort [1000000]
#+end_src
When run, the code executes initializes an array of the size given in
argument (1000000 by default) with random integer values and sorts it
using:
1. a custom sequential implementation;
2. a custom parallel implementation;
3. the libc qsort function.
Times are reported in seconds.

* Experimental Reports
** 2014-10-13                                                       :ARNAUD:
*** Initial code design
- I obtained an initial implementation from
  http://sc12.supercomputing.org/hpceducator/PythonForParallelism/codes/parallelQuicksort.c.
  According to the header, the original author is Joshua Stough from
  Washington and Lee University. I hope he will not mind that I reuse
  this piece of code for educational purposes.
- Here is a typical first execution on my laptop (an Intel(R) Core(TM)
  i7 running a Debian with Linux 3.14.15):
  #+begin_src sh :results output :exports both 
    ./src/quicksort
  #+end_src

  #+RESULTS:

  Sweet, in my first attempt, it looks like this parallel version is
  indeed running faster than then sequential one. I have to say this
  warning message is stressing me a bit though.
- On smaller instances, the code would segfault. So I reindented the
  code and thanks to valgrind and gdb, I could find what was wrong. I
  also renamed the file so that compiling is more convenient. This
  fixed the previous warning message so now everything seems fine:
  #+begin_src sh :results output :exports both 
    ./src/parallelQuicksort
  #+end_src

  #+RESULTS:
  : Sequential quicksort took: 0.239347 sec.
  : Parallel quicksort took: 0.176365 sec.
  : Built-in quicksort took: 0.244716 sec.

*** First series of experiments
Let's try to see how the three algorithms behave when changing the 
array size. Since one measurement is not enough, I run the code 5
times in a row.
#+begin_src sh foo :results output :exports both :tangle scripts/run_benchmarking.sh
  OUTPUT_DIRECTORY=data/`hostname`_`date +%F`
  mkdir -p $OUTPUT_DIRECTORY
  OUTPUT_FILE=$OUTPUT_DIRECTORY/measurements_`date +%R`.txt

  touch $OUTPUT_FILE
  for i in 100 1000 10000 100000 1000000; do
      for rep in `seq 1 5`; do
          echo "Size: $i" >> $OUTPUT_FILE;
          ./src/parallelQuicksort $i >> $OUTPUT_FILE;
      done ;
  done
#+end_src
I obtained the following [[file:data/sama_2014-10-13/measurements_03:47.txt][output]].

*** A simple plot with R                                       :REPRODUCED:
Here is a simple script to parse the results:
#+begin_src perl :results output raw :exports both :tangle scripts/csv_quicksort_extractor.pl
  use strict;

  my($line);
  my($size);

  print "Size, Type, Time\n" ;
  while($line=<>) {
      chomp $line;
      if($line =~/^Size: ([\d\.]*)$/) {
          $size = $1;
          next;
      } 
      if($line =~/^(.*) quicksort.*: ([\d\.]*) sec.$/) {
          print "$size, \"$1\", $2\n" ;
          next;
      } 
  }
#+end_src

I can then simply parse my data with the following command:

#+begin_src sh :results output :exports both 
perl scripts/csv_quicksort_extractor.pl < data/sama_2014-10-13/measurements_03\:47.txt > data/sama_2014-10-13/measurements_03\:47.csv
#+end_src

#+RESULTS:

#+begin_src R :results output graphics :file data/sama_2014-10-13/measurements_03:47.png :exports both :width 600 :height 400 :session
  df <- read.csv("data/sama_2014-10-13/measurements_03:47.csv",header=T)
  plot(df$Size,df$Time,col=c("red","blue","green")[df$Type])
#+end_src

#+RESULTS:
[[file:data/sama_2014-10-13/measurements_03:47.png]]

Well, this is not particularly nice and some may not know/like R.
*** A simple plot with gnuplot                                 :REPRODUCED:
So let's try to parse in an other way and use gnuplot:

#+begin_src perl :results output raw :exports both :tangle scripts/csv_quicksort_extractor2.pl
  use strict;

  my($line);
  my($size);
  my($seq,$par,$libc);
  print "Size, Seq, Par, Libc\n" ;
  while($line=<>) {
      chomp $line;
      if($line =~/^Size: ([\d\.]*)$/) {
          $size = $1;
          next;
      } 
      if($line =~/^Sequential quicksort.*: ([\d\.]*) sec.$/) {
          $seq=$1; next;
      } 
      if($line =~/^Parallel quicksort.*: ([\d\.]*) sec.$/) {
          $par=$1; next;
      } 
      if($line =~/^Built-in quicksort.*: ([\d\.]*) sec.$/) {
          $libc=$1; 
          print "$size, $seq, $pqr, $libc\n";
          next;
      }
  }
#+end_src

#+begin_src sh :results output raw :exports both 
  FILENAME="data/sama_2014-10-13/measurements_03:47"
  perl scripts/csv_quicksort_extractor2.pl < "$FILENAME.txt" > "${FILENAME}_wide.csv"
  echo "
    set terminal png size 600,400 
    set output '${FILENAME}_wide.png'
    set datafile separator ','
    set key autotitle columnhead
    plot '${FILENAME}_wide.csv' using 1:2 with linespoints, '' using 1:3 with linespoints, '' using 1:4 with linespoints
  " | gnuplot
  echo [[file:${FILENAME}_wide.png]]
#+end_src

#+RESULTS:
[[file:data/sama_2014-10-13/measurements_03:47_wide.png]]

Well, I'm not sure it is nicer but I have lines. A first crude
analysis seems to reveal the the parallel version is worth it for
arrays larger than 400000.
** 2014-10-25
*** Environment Setup
To play with org mode and easily reproduce Arnaud's analyses, I had to:
- Set-up emacs with a convenient configuration file (I based my configuration file on [[http://mescal.imag.fr/membres/arnaud.legrand/misc/init.org][the one]] on Arnaud's website).
- Install ess (and R, which is a dependency); on my Debian 3.16.3-2 I
  did this with the following command:
  #+BEGIN_SRC 
  sudo apt-get install ess 
  #+END_SRC
  After installing ess, I just added the following line to my emacs
  init file:
  #+begin_src emacs-lisp
  (require 'ess-site)
  #+end_src
*** Experimental environment (my personal machine)

I use the following script to get the system details.

#+begin_src sh :results output :tangle scripts/read_configuration.sh :shebang "#!/bin/bash"
echo "# OS details"
uname -a
echo "# HW details"
echo "Number of CPUs: "`cat /proc/cpuinfo | grep "model name" | wc -l`
echo "CPU information (all cpus are equal):"
echo "- "`grep "model name" /proc/cpuinfo | tail -1`
echo "- "`grep "cache size" /proc/cpuinfo | tail -1`
HT=`grep -o '^flags\b.*: .*\bht\b' /proc/cpuinfo | tail -1 | wc -l`
echo "- hyperthreading : "`if [ $HT -eq 1 ]; then echo "active"; else echo "not active"; fi` 
echo "Scaling governor: "`cat /sys/devices/system/cpu/cpu0/cpufreq/scaling_governor`
echo "RAM: "`cat /proc/meminfo | grep "MemTotal" | awk '{print $2 " " $3}'` 
#+end_src

#+begin_src sh :results output :exports results
./scripts/read_configuration.sh
#+end_src

#+RESULTS:
#+begin_example
# OS details
Linux jklaptop 3.16-2-amd64 #1 SMP Debian 3.16.3-2 (2014-09-20) x86_64 GNU/Linux
# HW details
Number of CPUs: 4
CPU information (all cpus are equal):
- model name : Intel(R) Core(TM) i7-2640M CPU @ 2.80GHz
- cache size : 4096 KB
- hyperthreading : active
Scaling governor: performance
RAM: 6040504 kB
#+end_example

*** First series of experiments
I run my first series of experiments varying the array size and
running 30 repetitions for each size.

#+begin_src sh :exports both :tangle scripts/bench_1.sh
  OUTPUT_DIRECTORY=data/`hostname`_`date +%F`
  mkdir -p $OUTPUT_DIRECTORY
  OUTPUT_FILE=$OUTPUT_DIRECTORY/measurements_`date +%R`.txt

  touch $OUTPUT_FILE
  for i in 100 1000 10000 100000 1000000; do
      for rep in `seq 1 30`; do
          echo "Size: $i" >> $OUTPUT_FILE;
          ./src/parallelQuicksort $i >> $OUTPUT_FILE;
      done ;
  done
  echo "finish"
#+end_src

#+RESULTS:
: finish

I obtained the following [[file:data/jklaptop_2014-10-25/measurements_21:13.txt][output]]. 
*** Analysis with R
I reuse Arnaud's perl script to parse the output into a csv format
that is easily readable using R.

#+begin_src sh :results output :exports both 
  FILENAME="data/jklaptop_2014-10-25/measurements_21:13"
  perl scripts/csv_quicksort_extractor.pl < "$FILENAME.txt" > "$FILENAME.csv"
#+end_src

#+RESULTS:

#+begin_src R :results output graphics :file data/jklaptop_2014-10-25/measurements_21:13.png :exports both :width 600 :height 400 
library("data.table")
library("ggplot2")
# create a summary: Size, Type, Mean, Error  
dt <- data.table(read.csv(""))
setkey(dt, Size, Type)
results = dt[,.(mean=mean(Time), sd=sd(Time), n=NROW(Time)), by="Size,Type"]
summary = data.frame(results[,.(Size, Type, mean, error = qnorm(0.975)*sd/sqrt(n))])
# plot
ggplot(summary, aes(x=Size, y=mean, colour=Type)) + 
    geom_errorbar(aes(ymin=mean-error, ymax=mean+error), width=.1) +
    geom_line() +
    geom_point() +
    scale_y_continuous(name="Time (s)") +
    scale_x_continuous(name="Array size")
#+end_src

#+RESULTS:
[[file:data/jklaptop_2014-10-25/measurements_21:13.png]]

Somewhere between 100000 and 1000000, the parallel version is
better. I want to do more experiments to see with better percision
when the parallel version of the algorithm actually becomes better.
** 2014-10-26
*** New series of experiments on my personal machine

I want to cover the region between 100k and 1M in this new experiments.
Then I'll combine the data with my previous results.

#+begin_src sh :exports both :tangle scripts/bench_2.sh
  OUTPUT_DIRECTORY=data/`hostname`_`date +%F`
  mkdir -p $OUTPUT_DIRECTORY
  OUTPUT_FILE=$OUTPUT_DIRECTORY/measurements_`date +%R`.txt

  touch $OUTPUT_FILE
  for i in 250000 500000 750000; do
      for rep in `seq 1 30`; do
          echo "Size: $i" >> $OUTPUT_FILE;
          ./src/parallelQuicksort $i >> $OUTPUT_FILE;
      done ;
  done
  echo "finish"
#+end_src

#+RESULTS:

*** New analysis with R

#+begin_src sh :results output :exports both 
  FILENAME="data/jklaptop_2014-10-26/measurements_19:59"
  perl scripts/csv_quicksort_extractor.pl < "$FILENAME.txt" > "$FILENAME.csv"
  cat "data/jklaptop_2014-10-25/measurements_21:13.csv" | grep -v "Size" >> "$FILENAME.csv"
#+end_src

#+RESULTS:

#+begin_src R :results output graphics :file data/jklaptop_2014-10-26/measurements_19:59.png :exports both :width 600 :height 400 
library("data.table")
library("ggplot2")
# create a summary: Size, Type, Mean, Error  
dt <- data.table(read.csv("data/jklaptop_2014-10-26/measurements_19:59.csv"))
setkey(dt, Size, Type)
results = dt[,.(mean=mean(Time), sd=sd(Time), n=NROW(Time)), by="Size,Type"]
summary = data.frame(results[,.(Size, Type, mean, error = qnorm(0.975)*sd/sqrt(n))])
# plot
ggplot(summary, aes(x=Size, y=mean, colour=Type)) + 
    geom_errorbar(aes(ymin=mean-error, ymax=mean+error), width=.1) +
    geom_line() +
    geom_point() +
    scale_y_continuous(name="Time (s)") +
    scale_x_continuous(name="Array size")
#+end_src

#+RESULTS:
[[file:data/jklaptop_2014-10-26/measurements_19:59.png]]

This analysis seems to reveal that, after 300k elements, the parallel
version of the quicksort starts to be better than the other versions.
By the way, the confidence intervals are quite big, so only after 750k
elements the parallel version actually shows a real gain.        

** 2014-11-02
*** Experiments on Grid-5k

After configuring a proxy command in my .ssh/config file, I prepare
the directory structure on the lille frontend, then I copy there the
source code, the read_configuration,sh script and a launch script.

Directory structure creation:

#+begin_src sh :session 5k 
ssh lille-g5k
mkdir -p generoso/quicksort/src
mkdir -p generoso/quicksort/data
mkdir -p generoso/quicksort/scripts
exit
#+end_src

Launch script:

#+begin_src sh foo :results output :exports both :tangle scripts/run_benchmarking_5k.sh :shebang "#!/bin/bash"
  OUTPUT_DIRECTORY=data/`hostname`_`date +%F`
  mkdir -p $OUTPUT_DIRECTORY
  OUTPUT_FILE=$OUTPUT_DIRECTORY/measurements_`date +%R`.txt
  CONF_FILE=$OUTPUT_DIRECTORY/configuration_`date +%R`.txt

  touch $CONF_FILE
  ./scripts/read_configuration.sh >> $CONF_FILE

  touch $OUTPUT_FILE
  for i in 100 1000 10000 100000 250000 500000 750000 1000000; do
      for rep in `seq 1 5`; do
          echo "Size: $i" >> $OUTPUT_FILE;
          ./src/parallelQuicksort $i >> $OUTPUT_FILE;
      done ;
  done
#+end_src

Copy src and scripts:

#+begin_src sh :session 5k
scp -r src/Makefile gpagano@lille-g5k:/home/gpagano/generoso/quicksort/src
scp -r src/parallelQuicksort.c gpagano@lille-g5k:/home/gpagano/generoso/quicksort/src
scp -r scripts/run_benchmarking_5k.sh gpagano@lille-g5k:/home/gpagano/generoso/quicksort/scripts
scp -r scripts/read_configuration.sh gpagano@lille-g5k:/home/gpagano/generoso/quicksort/scripts
#+end_src

Connect to the lille frontend, reserve a node, compile the code and
run the experiments.

#+begin_src sh :results output :session 5k
ssh lille-g5k
oarsub -I
cd generoso/quicksort
make -C ./src
./scripts/run_benchmarking_5k.sh
#+end_src

Get results back for analysis:
  
#+begin_src sh :results output :session 5k
scp -r lille-g5k:/home/gpagano/generoso/quicksort/data/* data
#+end_src

*** Analysis of Grid-5k results

The node used for the experiments is chirloute-3.lille.grid5000.fr.
The configuration is the following:

#+begin_src sh :results output :exports results
cat ./data/chirloute-3.lille.grid5000.fr_2014-11-02/configuration_15\:37.txt
#+end_src

#+RESULTS:
#+begin_example
# OS details
Linux chirloute-3.lille.grid5000.fr 3.2.0-4-amd64 #1 SMP Debian 3.2.60-1+deb7u3 x86_64 GNU/Linux
# HW details
Number of CPUs: 8
CPU information (all cpus are equal):
- model name : Intel(R) Xeon(R) CPU E5620 @ 2.40GHz
- cache size : 12288 KB
- hyperthreading : active
Scaling governor: performance
RAM: 8188956 kB
#+end_example

#+begin_src sh :results output :exports both 
  FILENAME="data/chirloute-3.lille.grid5000.fr_2014-11-02/measurements_15:37"
  perl scripts/csv_quicksort_extractor.pl < "$FILENAME.txt" > "$FILENAME.csv"
#+end_src

#+RESULTS:

#+begin_src R :results output graphics :file data/chirloute-3.lille.grid5000.fr_2014-11-02/measurements_15:37.png :exports both :width 600 :height 400 
library("data.table")
library("ggplot2")
# create a summary: Size, Type, Mean, Error  
dt <- data.table(read.csv("data/chirloute-3.lille.grid5000.fr_2014-11-02/measurements_15:37.csv"))
setkey(dt, Size, Type)
results = dt[,.(mean=mean(Time), sd=sd(Time), n=NROW(Time)), by="Size,Type"]
summary = data.frame(results[,.(Size, Type, mean, error = qnorm(0.975)*sd/sqrt(n))])
# plot
ggplot(summary, aes(x=Size, y=mean, colour=Type)) + 
    geom_errorbar(aes(ymin=mean-error, ymax=mean+error), width=.1) +
    geom_line() +
    geom_point() +
    scale_y_continuous(name="Time (s)") +
    scale_x_continuous(name="Array size")
#+end_src

#+RESULTS:
[[file:data/chirloute-3.lille.grid5000.fr_2014-11-02/measurements_15:37.png]]

On the chirloute node, the parallel version of quicksort seems to be
actually worth it only starting from about 1000000 (considering the
confidence interval).
